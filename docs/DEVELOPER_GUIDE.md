# Developer Guide - Supply Matriz de Merecimento

## üìã Vis√£o Geral

Este guia t√©cnico fornece informa√ß√µes detalhadas para desenvolvedores que trabalham com o sistema de matriz de merecimento, incluindo arquitetura, padr√µes de c√≥digo, e melhores pr√°ticas.

## üèóÔ∏è Arquitetura do Sistema

### **Estrutura Modular**

O sistema √© constru√≠do com uma arquitetura modular que separa responsabilidades:

```
src/
‚îú‚îÄ‚îÄ calculo_matriz_de_merecimento_unificado.py  # N√∫cleo do sistema
‚îú‚îÄ‚îÄ Preparacao_tabelas_Matriz_merecimento.py    # Prepara√ß√£o de dados
‚îú‚îÄ‚îÄ Salvar_matrizes_calculadas_csv.py           # Exporta√ß√£o
‚îî‚îÄ‚îÄ analysis/                                   # An√°lises e utilit√°rios
    ‚îú‚îÄ‚îÄ Analise_demanda_matriz_telas.py
    ‚îú‚îÄ‚îÄ Analise_demanda_matriz_antiga.py
    ‚îú‚îÄ‚îÄ analise_elasticidade_demanda.py
    ‚îú‚îÄ‚îÄ analise_elasticidade_eventos.py
    ‚îú‚îÄ‚îÄ analise_factual_comparacao_matrizes.py
    ‚îî‚îÄ‚îÄ analise_resultados_factuais.py
```

### **Padr√µes de Design**

#### **1. Fun√ß√µes Modulares**
- **Responsabilidade √∫nica**: Cada fun√ß√£o tem um prop√≥sito espec√≠fico
- **Reutiliza√ß√£o**: Fun√ß√µes podem ser reutilizadas em diferentes contextos
- **Testabilidade**: Fun√ß√µes isoladas s√£o mais f√°ceis de testar

#### **2. Configura√ß√£o Centralizada**
```python
REGRAS_AGRUPAMENTO = {
    "DIRETORIA DE TELAS": {
        "coluna_grupo_necessidade": "gemeos",
        "tipo_agrupamento": "g√™meos",
        "descricao": "Agrupamento por produtos similares (g√™meos)"
    }
    # ... outras categorias
}
```

#### **3. Tratamento de Erros**
- **Valida√ß√£o de entrada**: Verifica√ß√£o de par√¢metros
- **Logs informativos**: Mensagens claras para debugging
- **Fallbacks**: Valores padr√£o quando apropriado

## üîß Padr√µes de C√≥digo

### **Python Standards**

#### **Type Hints**
```python
def executar_calculo_matriz_merecimento(
    categoria: str,
    data_inicio: str = "2024-01-01",
    sigma_meses_atipicos: float = 3.0
) -> DataFrame:
    """Executa o c√°lculo da matriz de merecimento."""
    pass
```

#### **Docstrings**
```python
def calcular_medidas_centrais_com_medias_aparadas(
    df: DataFrame,
    janelas_moveis: List[int],
    percentual_corte: float = 0.10
) -> DataFrame:
    """
    Calcula medidas centrais incluindo m√©dias aparadas.
    
    Args:
        df: DataFrame com dados de vendas
        janelas_moveis: Lista de janelas m√≥veis em dias
        percentual_corte: Percentual de corte para m√©dias aparadas
        
    Returns:
        DataFrame com medidas centrais calculadas
    """
    pass
```

#### **Nomenclatura**
- **Vari√°veis**: `snake_case` (ex: `df_resultado`, `total_vendas`)
- **Fun√ß√µes**: `snake_case` (ex: `calcular_merecimento_cd`)
- **Constantes**: `UPPER_CASE` (ex: `JANELAS_MOVEIS`)
- **Classes**: `PascalCase` (ex: `MatrizMerecimento`)

### **PySpark Standards**

#### **DataFrame Operations**
```python
# Usar method chaining quando apropriado
df_resultado = (
    df_base
    .filter(F.col("categoria") == categoria)
    .groupBy("grupo_necessidade")
    .agg(F.sum("quantidade").alias("total"))
    .orderBy(F.desc("total"))
)
```

#### **Window Functions**
```python
# Definir janelas para c√°lculos m√≥veis
window_spec = Window.partitionBy("grupo_necessidade").orderBy("data").rowsBetween(-89, 0)

df_com_media = df_base.withColumn(
    "media_90_dias",
    F.avg("quantidade").over(window_spec)
)
```

#### **Cache Strategy**
```python
# Cache DataFrames que s√£o reutilizados
df_base.cache()
df_mapeamentos.cache()

# Unpersist quando n√£o precisar mais
df_base.unpersist()
```

## üß™ Testing

### **Estrutura de Testes**

```
tests/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ test_calculo_matriz.py
‚îú‚îÄ‚îÄ test_medidas_centrais.py
‚îú‚îÄ‚îÄ test_deteccao_outliers.py
‚îî‚îÄ‚îÄ test_analise_elasticidade.py
```

### **Testes Unit√°rios**

```python
import unittest
from pyspark.sql import SparkSession
from src.calculo_matriz_de_merecimento_unificado import calcular_medidas_centrais

class TestMedidasCentrais(unittest.TestCase):
    def setUp(self):
        self.spark = SparkSession.builder.appName("test").getOrCreate()
        
    def test_calculo_media_90_dias(self):
        # Arrange
        df_teste = self.spark.createDataFrame([
            (1, "2024-01-01", 10),
            (1, "2024-01-02", 20)
        ], ["sku", "data", "quantidade"])
        
        # Act
        resultado = calcular_medidas_centrais(df_teste, [90])
        
        # Assert
        self.assertEqual(resultado.count(), 2)
        
    def tearDown(self):
        self.spark.stop()
```

### **Testes de Integra√ß√£o**

```python
def test_fluxo_completo_matriz_telas():
    """Testa o fluxo completo para categoria de telas."""
    # Arrange
    categoria = "DIRETORIA DE TELAS"
    
    # Act
    resultado = executar_calculo_matriz_merecimento(categoria)
    
    # Assert
    assert resultado.count() > 0
    assert "grupo_de_necessidade" in resultado.columns
    assert "Merecimento_Final_Media90_Qt_venda_sem_ruptura" in resultado.columns
```

## üìä Performance

### **Otimiza√ß√µes PySpark**

#### **1. Particionamento**
```python
# Particionar por colunas frequentemente usadas em joins
df_base = df_base.repartition("cdfilial", "grupo_necessidade")
```

#### **2. Broadcast Joins**
```python
# Para tabelas pequenas de mapeamento
df_mapeamentos = broadcast(df_mapeamentos)
df_resultado = df_base.join(df_mapeamentos, "sku", "left")
```

#### **3. Filtros Precoces**
```python
# Aplicar filtros antes de opera√ß√µes custosas
df_filtrado = (
    df_base
    .filter(F.col("data") >= data_inicio)
    .filter(F.col("categoria") == categoria)
)
```

### **Monitoramento de Performance**

#### **Logs de Performance**
```python
import time

def executar_com_timing(func, *args, **kwargs):
    """Executa fun√ß√£o com medi√ß√£o de tempo."""
    inicio = time.time()
    resultado = func(*args, **kwargs)
    fim = time.time()
    
    print(f"Execu√ß√£o de {func.__name__}: {fim - inicio:.2f} segundos")
    return resultado
```

#### **M√©tricas de Cluster**
```python
# Monitorar uso de recursos
def monitorar_recursos():
    """Monitora uso de recursos do cluster."""
    print(f"Executors ativos: {spark.sparkContext.statusTracker().getExecutorInfos()}")
    print(f"Jobs ativos: {spark.sparkContext.statusTracker().getJobIdsForGroup()}")
```

## üîç Debugging

### **Logs Estruturados**

```python
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def carregar_dados_base(categoria: str) -> DataFrame:
    """Carrega dados base com logging."""
    logger.info(f"Carregando dados base para categoria: {categoria}")
    
    try:
        df = spark.table("databox.bcg_comum.supply_base_merecimento_diario")
        logger.info(f"Dados carregados: {df.count()} registros")
        return df
    except Exception as e:
        logger.error(f"Erro ao carregar dados: {str(e)}")
        raise
```

### **Valida√ß√£o de Dados**

```python
def validar_dataframe(df: DataFrame, nome: str) -> None:
    """Valida DataFrame com verifica√ß√µes b√°sicas."""
    logger.info(f"Validando DataFrame: {nome}")
    
    # Verificar se tem dados
    if df.count() == 0:
        raise ValueError(f"DataFrame {nome} est√° vazio")
    
    # Verificar colunas essenciais
    colunas_essenciais = ["cdfilial", "cd_primario", "grupo_necessidade"]
    for col in colunas_essenciais:
        if col not in df.columns:
            raise ValueError(f"Coluna {col} n√£o encontrada em {nome}")
    
    logger.info(f"DataFrame {nome} validado com sucesso")
```

### **Debugging de Queries**

```python
# Habilitar explain para debugging
df.explain(True)

# Usar show() para inspecionar dados
df.show(20, truncate=False)

# Verificar schema
df.printSchema()
```

## üöÄ Deployment

### **Ambiente Databricks**

#### **Configura√ß√£o do Cluster**
```python
# Configura√ß√µes recomendadas
cluster_config = {
    "num_workers": 4,
    "node_type_id": "i3.xlarge",
    "spark_version": "13.3.x-scala2.12",
    "spark_conf": {
        "spark.sql.adaptive.enabled": "true",
        "spark.sql.adaptive.coalescePartitions.enabled": "true"
    }
}
```

#### **Bibliotecas Necess√°rias**
```python
# Bibliotecas Python
libraries = [
    "pandas>=1.5.0",
    "plotly>=5.0.0",
    "openpyxl>=3.0.0"
]
```

### **Pipeline de CI/CD**

#### **GitHub Actions**
```yaml
name: Test and Deploy
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.9
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run tests
        run: python -m pytest tests/
```

## üìö Documenta√ß√£o

### **Padr√µes de Documenta√ß√£o**

#### **README Files**
- **README.md**: Documenta√ß√£o principal do projeto
- **README.md** (pasta analysis): Documenta√ß√£o espec√≠fica da pasta
- **README_ELASTICIDADE_DATABRICKS.md**: Documenta√ß√£o espec√≠fica para Databricks

#### **Docstrings**
```python
def calcular_merecimento_cd(
    df: DataFrame,
    grupo_necessidade: str,
    medida: str
) -> DataFrame:
    """
    Calcula merecimento a n√≠vel de Centro de Distribui√ß√£o.
    
    Esta fun√ß√£o implementa a primeira camada do c√°lculo de merecimento,
    agregando demanda por CD e grupo de necessidade.
    
    Args:
        df: DataFrame com dados de vendas por loja
        grupo_necessidade: Nome da coluna de agrupamento
        medida: Nome da coluna de medida (ex: 'Media90_Qt_venda_sem_ruptura')
        
    Returns:
        DataFrame com merecimento agregado por CD
        
    Raises:
        ValueError: Se grupo_necessidade n√£o for encontrado
        KeyError: Se medida n√£o for encontrada
        
    Example:
        >>> df_resultado = calcular_merecimento_cd(df_vendas, 'gemeos', 'Media90_Qt_venda_sem_ruptura')
        >>> df_resultado.show()
    """
    pass
```

### **Coment√°rios no C√≥digo**

```python
# Configura√ß√£o de par√¢metros para detec√ß√£o de outliers
# Valores baseados em an√°lise estat√≠stica de dados hist√≥ricos
PARAMETROS_OUTLIERS = {
    "desvios_meses_atipicos": 3,  # 3œÉ para meses at√≠picos
    "desvios_historico_cd": 3,     # 3œÉ para outliers CD
    "desvios_historico_loja": 3,   # 3œÉ para outliers loja
    "desvios_atacado_cd": 1.5,     # 1.5œÉ para atacado CD (mais restritivo)
    "desvios_atacado_loja": 1.5    # 1.5œÉ para atacado loja (mais restritivo)
}
```

## üîß Manuten√ß√£o

### **Versionamento**

#### **Semantic Versioning**
- **MAJOR**: Mudan√ßas incompat√≠veis na API
- **MINOR**: Novas funcionalidades compat√≠veis
- **PATCH**: Corre√ß√µes de bugs

#### **Changelog**
```markdown
## [2.0.0] - 2025-01-XX

### Added
- Sistema unificado para todas as categorias
- An√°lise de elasticidade de demanda
- M√©tricas de qualidade da matriz

### Changed
- Refatora√ß√£o completa da arquitetura
- Melhoria na detec√ß√£o de outliers
- Otimiza√ß√£o de performance

### Fixed
- Corre√ß√£o de paths para execu√ß√£o no Databricks
- Corre√ß√£o de bugs na detec√ß√£o de outliers
- Melhoria na valida√ß√£o de dados
```

### **Monitoramento**

#### **M√©tricas de Sistema**
```python
def coletar_metricas_sistema():
    """Coleta m√©tricas do sistema para monitoramento."""
    return {
        "timestamp": datetime.now().isoformat(),
        "total_skus": df_skus.count(),
        "total_lojas": df_lojas.count(),
        "total_grupos": df_grupos.count(),
        "tempo_execucao": tempo_execucao,
        "memoria_utilizada": memoria_utilizada
    }
```

## ü§ù Contribui√ß√£o

### **Processo de Contribui√ß√£o**

1. **Fork** do reposit√≥rio
2. **Criar branch** para feature/fix
3. **Implementar** com testes
4. **Documentar** mudan√ßas
5. **Pull request** com descri√ß√£o detalhada
6. **Code review** obrigat√≥rio
7. **Merge** ap√≥s aprova√ß√£o

### **Code Review Checklist**

- [ ] C√≥digo segue padr√µes estabelecidos
- [ ] Testes passam
- [ ] Documenta√ß√£o atualizada
- [ ] Performance considerada
- [ ] Tratamento de erros implementado
- [ ] Logs apropriados adicionados

## üìû Suporte

### **Canais de Suporte**

1. **Documenta√ß√£o**: Consultar READMEs e docstrings
2. **Issues**: GitHub Issues para bugs e features
3. **Discuss√µes**: GitHub Discussions para d√∫vidas
4. **Email**: Contato direto com equipe

### **Informa√ß√µes para Suporte**

Ao solicitar suporte, incluir:
- **Vers√£o** do sistema
- **Ambiente** (Databricks, local)
- **Logs** de erro
- **Dados** de exemplo (se aplic√°vel)
- **Passos** para reproduzir o problema

---

**Vers√£o**: 2.0.0  
**√öltima Atualiza√ß√£o**: Janeiro 2025  
**Mantenedor**: Equipe de Supply Chain Analytics
