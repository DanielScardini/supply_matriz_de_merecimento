# Installation Guide - Supply Matriz de Merecimento

## ðŸ“‹ VisÃ£o Geral

Este guia fornece instruÃ§Ãµes detalhadas para instalaÃ§Ã£o e configuraÃ§Ã£o do sistema de matriz de merecimento no ambiente Databricks.

## ðŸŽ¯ PrÃ©-requisitos

### **Ambiente Databricks**

#### **VersÃ£o MÃ­nima**
- **Databricks Runtime**: 13.3 LTS ou superior
- **Python**: 3.9 ou superior
- **Scala**: 2.12 ou superior

#### **PermissÃµes NecessÃ¡rias**
- **Acesso Ã s tabelas**: `databox.bcg_comum.*`
- **CriaÃ§Ã£o de tabelas**: Para salvar resultados
- **Acesso ao DBFS**: Para arquivos de mapeamento
- **ExecuÃ§Ã£o de jobs**: Para automaÃ§Ã£o

### **Recursos de Cluster**

#### **ConfiguraÃ§Ã£o MÃ­nima**
```json
{
  "num_workers": 2,
  "node_type_id": "i3.xlarge",
  "driver_node_type_id": "i3.xlarge",
  "spark_version": "13.3.x-scala2.12"
}
```

#### **ConfiguraÃ§Ã£o Recomendada**
```json
{
  "num_workers": 4,
  "node_type_id": "i3.2xlarge",
  "driver_node_type_id": "i3.2xlarge",
  "spark_version": "13.3.x-scala2.12",
  "spark_conf": {
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.adaptive.coalescePartitions.enabled": "true",
    "spark.sql.adaptive.skewJoin.enabled": "true"
  }
}
```

## ðŸš€ InstalaÃ§Ã£o

### **1. PreparaÃ§Ã£o do Ambiente**

#### **Criar Workspace**
1. Acesse o Databricks workspace
2. Crie um novo workspace ou use existente
3. Configure permissÃµes de acesso Ã s tabelas

#### **Configurar Cluster**
1. VÃ¡ para **Compute** â†’ **Clusters**
2. Clique em **Create Cluster**
3. Configure conforme especificaÃ§Ãµes acima
4. Salve e inicie o cluster

### **2. Upload do CÃ³digo**

#### **MÃ©todo 1: Git Integration**
```bash
# Clonar repositÃ³rio
git clone https://github.com/your-org/supply_matriz_de_merecimento.git

# Configurar Git no Databricks
# VÃ¡ para User Settings â†’ Git Integration
# Adicione suas credenciais Git
```

#### **MÃ©todo 2: Upload Manual**
1. VÃ¡ para **Workspace** â†’ **Users** â†’ **Seu UsuÃ¡rio**
2. Crie pasta `supply_matriz_de_merecimento`
3. Upload dos arquivos Python
4. Organize conforme estrutura do projeto

### **3. ConfiguraÃ§Ã£o de Bibliotecas**

#### **Bibliotecas Python**
```python
# Instalar via pip no notebook
%pip install pandas>=1.5.0
%pip install plotly>=5.0.0
%pip install openpyxl>=3.0.0
%pip install tqdm>=4.64.0
```

#### **Bibliotecas via Cluster**
1. VÃ¡ para **Compute** â†’ **Clusters** â†’ **Seu Cluster**
2. Clique em **Libraries** â†’ **Install New**
3. Selecione **PyPI** e instale:
   - `pandas>=1.5.0`
   - `plotly>=5.0.0`
   - `openpyxl>=3.0.0`
   - `tqdm>=4.64.0`

### **4. ConfiguraÃ§Ã£o de Dados**

#### **Arquivos de Mapeamento**
1. Crie pasta no DBFS: `/dbfs/dados_analise/`
2. Upload dos arquivos CSV:
   - `MODELOS_AJUSTE (1).csv`
   - `ITENS_GEMEOS 2.csv`
   - `(DRP)_MATRIZ_*.csv`

#### **Estrutura de Pastas**
```
/dbfs/
â”œâ”€â”€ dados_analise/
â”‚   â”œâ”€â”€ MODELOS_AJUSTE (1).csv
â”‚   â”œâ”€â”€ ITENS_GEMEOS 2.csv
â”‚   â”œâ”€â”€ (DRP)_MATRIZ_20250829135142.csv
â”‚   â””â”€â”€ (DRP)_MATRIZ_20250902160333.csv
â””â”€â”€ resultados/
    â”œâ”€â”€ matriz_telas/
    â”œâ”€â”€ matriz_telefonia/
    â”œâ”€â”€ matriz_linha_branca/
    â”œâ”€â”€ matriz_linha_leve/
    â””â”€â”€ matriz_info_games/
```

## âš™ï¸ ConfiguraÃ§Ã£o

### **1. ConfiguraÃ§Ã£o de Paths**

#### **Atualizar Paths nos Arquivos**
```python
# Em cada arquivo de anÃ¡lise, atualizar paths para:
BASE_PATH = "/dbfs/dados_analise/"
RESULTADOS_PATH = "/dbfs/resultados/"

# Exemplo de atualizaÃ§Ã£o
df_modelos = pd.read_csv(f"{BASE_PATH}MODELOS_AJUSTE (1).csv")
```

#### **ConfiguraÃ§Ã£o de Tabelas**
```python
# Configurar nomes das tabelas
TABELA_BASE = "databox.bcg_comum.supply_base_merecimento_diario_v3"
TABELA_LOJAS = "data_engineering_prd.app_operacoesloja.roteirizacaolojaativa"
```

### **2. ConfiguraÃ§Ã£o de ParÃ¢metros**

#### **ParÃ¢metros por Categoria**
```python
# Configurar parÃ¢metros especÃ­ficos por categoria
CONFIGURACOES_CATEGORIA = {
    "DIRETORIA DE TELAS": {
        "sigma_meses_atipicos": 2.0,
        "sigma_outliers_cd": 2.5,
        "sigma_outliers_loja": 3.0
    },
    "DIRETORIA TELEFONIA CELULAR": {
        "sigma_meses_atipicos": 2.0,
        "sigma_outliers_cd": 2.5,
        "sigma_outliers_loja": 3.0
    },
    "DIRETORIA LINHA BRANCA": {
        "sigma_meses_atipicos": 3.0,
        "sigma_outliers_cd": 3.0,
        "sigma_outliers_loja": 3.0
    }
}
```

#### **ConfiguraÃ§Ã£o de Datas**
```python
# Configurar datas de execuÃ§Ã£o
DATA_INICIO = "2024-01-01"
DATA_CALCULO = "2025-06-30"
DATA_FIM = "2025-12-31"
```

### **3. ConfiguraÃ§Ã£o de Logs**

#### **Configurar Logging**
```python
import logging

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/dbfs/logs/matriz_merecimento.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
```

## ðŸ§ª Testes

### **1. Teste de Conectividade**

#### **Verificar Acesso Ã s Tabelas**
```python
# Teste de conectividade
def testar_conectividade():
    try:
        # Testar tabela base
        df_base = spark.table("databox.bcg_comum.supply_base_merecimento_diario_v3")
        print(f"âœ… Tabela base acessÃ­vel: {df_base.count()} registros")
        
        # Testar tabela de lojas
        df_lojas = spark.table("data_engineering_prd.app_operacoesloja.roteirizacaolojaativa")
        print(f"âœ… Tabela de lojas acessÃ­vel: {df_lojas.count()} registros")
        
        return True
    except Exception as e:
        print(f"âŒ Erro de conectividade: {str(e)}")
        return False

# Executar teste
testar_conectividade()
```

#### **Verificar Arquivos de Mapeamento**
```python
# Teste de arquivos
def testar_arquivos():
    import os
    
    arquivos = [
        "/dbfs/dados_analise/MODELOS_AJUSTE (1).csv",
        "/dbfs/dados_analise/ITENS_GEMEOS 2.csv"
    ]
    
    for arquivo in arquivos:
        if os.path.exists(arquivo):
            print(f"âœ… Arquivo encontrado: {arquivo}")
        else:
            print(f"âŒ Arquivo nÃ£o encontrado: {arquivo}")

# Executar teste
testar_arquivos()
```

### **2. Teste de ExecuÃ§Ã£o**

#### **Teste com Dados Pequenos**
```python
# Teste com amostra pequena
def testar_execucao_pequena():
    try:
        # Executar com dados limitados
        df_teste = executar_calculo_matriz_merecimento(
            categoria="DIRETORIA DE TELAS",
            data_inicio="2024-12-01",
            data_calculo="2024-12-31"
        )
        
        print(f"âœ… ExecuÃ§Ã£o bem-sucedida: {df_teste.count()} registros")
        return True
    except Exception as e:
        print(f"âŒ Erro na execuÃ§Ã£o: {str(e)}")
        return False

# Executar teste
testar_execucao_pequena()
```

## ðŸš€ ExecuÃ§Ã£o

### **1. ExecuÃ§Ã£o Manual**

#### **Notebook Principal**
```python
# Executar cÃ¡lculo para todas as categorias
categorias = [
    "DIRETORIA DE TELAS",
    "DIRETORIA TELEFONIA CELULAR",
    "DIRETORIA LINHA BRANCA",
    "DIRETORIA LINHA LEVE",
    "DIRETORIA INFO/GAMES"
]

for categoria in categorias:
    print(f"Processando {categoria}...")
    df_resultado = executar_calculo_matriz_merecimento(categoria)
    
    # Salvar resultado
    df_resultado.write.mode("overwrite").parquet(f"/dbfs/resultados/matriz_{categoria.lower().replace(' ', '_')}/")
    print(f"âœ… {categoria} concluÃ­do")
```

#### **ExecuÃ§Ã£o de AnÃ¡lises**
```python
# Executar anÃ¡lises de elasticidade
executar_analise_elasticidade_demanda()

# Executar anÃ¡lises factuais
executar_analise_factual_comparacao_matrizes()
```

### **2. AutomaÃ§Ã£o com Jobs**

#### **Criar Job no Databricks**
1. VÃ¡ para **Workflows** â†’ **Jobs**
2. Clique em **Create Job**
3. Configure:
   - **Name**: `Matriz de Merecimento - CÃ¡lculo DiÃ¡rio`
   - **Task Type**: `Notebook`
   - **Source**: Seu notebook principal
   - **Cluster**: Seu cluster configurado

#### **Configurar Schedule**
```python
# Configurar execuÃ§Ã£o diÃ¡ria
schedule = {
    "quartz_cron_expression": "0 0 6 * * ?",  # 6:00 AM diariamente
    "timezone_id": "America/Sao_Paulo",
    "pause_status": "UNPAUSED"
}
```

### **3. Monitoramento**

#### **Configurar Alertas**
```python
# Configurar alertas para falhas
def configurar_alertas():
    # Alertas por email
    alertas = {
        "email": "supply-team@empresa.com",
        "webhook": "https://hooks.slack.com/...",
        "condicoes": [
            "tempo_execucao > 2_horas",
            "erro_count > 0",
            "registros_processados < 1000"
        ]
    }
    return alertas
```

## ðŸ”§ Troubleshooting

### **Problemas Comuns**

#### **1. Erro de PermissÃ£o**
```
Error: Table or view not found: databox.bcg_comum.supply_base_merecimento_diario_v3
```
**SoluÃ§Ã£o**: Verificar permissÃµes de acesso Ã s tabelas

#### **2. Erro de MemÃ³ria**
```
Error: OutOfMemoryError: Java heap space
```
**SoluÃ§Ã£o**: Aumentar tamanho do cluster ou otimizar queries

#### **3. Erro de Arquivo**
```
Error: File not found: /dbfs/dados_analise/MODELOS_AJUSTE (1).csv
```
**SoluÃ§Ã£o**: Verificar se arquivos foram uploadados corretamente

### **Logs de Debug**

#### **Habilitar Logs Detalhados**
```python
# Configurar logging detalhado
import logging
logging.getLogger("py4j").setLevel(logging.DEBUG)
logging.getLogger("pyspark").setLevel(logging.DEBUG)
```

#### **Verificar Logs do Cluster**
1. VÃ¡ para **Compute** â†’ **Clusters** â†’ **Seu Cluster**
2. Clique em **Logs**
3. Verifique **Driver Logs** e **Worker Logs**

## ðŸ“Š Performance

### **OtimizaÃ§Ãµes Recomendadas**

#### **1. ConfiguraÃ§Ã£o de Cluster**
```python
# ConfiguraÃ§Ãµes de performance
spark_conf = {
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.adaptive.coalescePartitions.enabled": "true",
    "spark.sql.adaptive.skewJoin.enabled": "true",
    "spark.sql.adaptive.localShuffleReader.enabled": "true",
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer"
}
```

#### **2. Particionamento de Dados**
```python
# Particionar dados por categoria
df_base = df_base.repartition("categoria", "cdfilial")
```

#### **3. Cache EstratÃ©gico**
```python
# Cache de DataFrames reutilizados
df_base.cache()
df_mapeamentos.cache()
```

## ðŸ”’ SeguranÃ§a

### **ConfiguraÃ§Ãµes de SeguranÃ§a**

#### **1. Acesso a Dados**
- **PrincÃ­pio do menor privilÃ©gio**: Apenas permissÃµes necessÃ¡rias
- **Auditoria**: Log de acessos e modificaÃ§Ãµes
- **Criptografia**: Dados sensÃ­veis criptografados

#### **2. Controle de Acesso**
```python
# Configurar controle de acesso
def verificar_permissao(usuario, operacao):
    """Verifica se usuÃ¡rio tem permissÃ£o para operaÃ§Ã£o."""
    permissoes = {
        "admin": ["read", "write", "execute"],
        "analyst": ["read", "execute"],
        "viewer": ["read"]
    }
    return operacao in permissoes.get(usuario, [])
```

## ðŸ“š DocumentaÃ§Ã£o Adicional

### **Recursos Ãšteis**

- **Databricks Documentation**: https://docs.databricks.com/
- **PySpark Documentation**: https://spark.apache.org/docs/latest/api/python/
- **Pandas Documentation**: https://pandas.pydata.org/docs/

### **Suporte**

Para problemas de instalaÃ§Ã£o ou configuraÃ§Ã£o:

1. **Verificar logs** de erro
2. **Consultar documentaÃ§Ã£o** do Databricks
3. **Contatar equipe** de suporte
4. **Abrir issue** no repositÃ³rio

---

**VersÃ£o**: 2.0.0  
**Ãšltima AtualizaÃ§Ã£o**: Janeiro 2025  
**Mantenedor**: Equipe de Supply Chain Analytics
