# Databricks notebook source
# MAGIC %md
# MAGIC # Salvamento de Matrizes de Merecimento - Formato Sistema de Abastecimento
# MAGIC
# MAGIC Este notebook implementa o salvamento de matrizes em formato CSV compat√≠vel com o sistema de abastecimento.
# MAGIC
# MAGIC **Especifica√ß√µes:**
# MAGIC - Formato: CSV sem index
# MAGIC - Colunas: SKU, CANAL, LOJA, DATA FIM, PERCENTUAL, VERIFICAR, FASE DE VIDA
# MAGIC - Uni√£o de ONLINE e OFFLINE no mesmo arquivo
# MAGIC - M√°ximo 200.000 linhas por arquivo
# MAGIC - Mesmo SKU-FILIAL sempre no mesmo arquivo (ambos canais)
# MAGIC - Normaliza√ß√£o para exatamente 100.00% por CdSku
# MAGIC - Ajuste de diferen√ßa no maior merecimento

# COMMAND ----------

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F, Window as W
from datetime import datetime, timedelta
import os
import pandas as pd
from typing import List, Dict, Tuple

!pip install openpyxl

# Inicializa√ß√£o
spark = SparkSession.builder.appName("salvar_matrizes_csv_sistema").getOrCreate()

# Datas
DATA_ATUAL = datetime.now()
DATA_FIM = DATA_ATUAL + timedelta(days=60)
DATA_FIM_INT = int(DATA_FIM.strftime("%Y%m%d"))

print(f"üìÖ Data atual: {DATA_ATUAL.strftime('%Y-%m-%d')}")
print(f"üìÖ Data fim (+60 dias): {DATA_FIM.strftime('%Y-%m-%d')} ‚Üí {DATA_FIM_INT}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Configura√ß√µes

# COMMAND ----------

dt_inicio = "2025-08-01"
dt_fim = "2025-10-01"

# Calcular top 80% por ESP√âCIE (SKUs) - apenas PORTATEIS
df_demanda_especie = (
  spark.table('databox.bcg_comum.supply_base_merecimento_diario_v4')
  .filter(F.col("NmSetorGerencial") == "PORTATEIS")
  .filter(F.col("DtAtual") >= dt_inicio)
  .filter(F.col("DtAtual") < dt_fim)
  .groupBy("NmEspecieGerencial")
  .agg(
    F.sum(F.col("QtMercadoria")).alias("QtDemanda"),
    F.sum(F.col("Receita")).alias("Receita")
  )
)

# calcular totais com window
w_total = W.rowsBetween(W.unboundedPreceding, W.unboundedFollowing)

# window para cumulativo
w_cum = W.orderBy(F.col("PercDemanda").desc()).rowsBetween(W.unboundedPreceding, 0)

# TOP 80% POR ESP√âCIE (SKUs) - apenas PORTATEIS
df_demanda_especie = (
    df_demanda_especie
    .withColumn("TotalDemanda", F.sum("QtDemanda").over(w_total))
    .withColumn("TotalReceita", F.sum("Receita").over(w_total))
    .withColumn("PercDemanda", F.round((F.col("QtDemanda") / F.col("TotalDemanda")) * 100, 0))
    .withColumn("PercReceita", F.round((F.col("Receita") / F.col("TotalReceita")) * 100, 0))
    .drop("TotalDemanda", "TotalReceita")
    .withColumn("PercDemandaCumulativo", F.sum("PercDemanda").over(w_cum))
    .withColumn("PercReceitaCumulativo", F.sum("PercReceita").over(w_cum))
)

especies_top80 = (
    df_demanda_especie
    .filter(F.col("PercDemandaCumulativo") <= 80)
    .select("NmEspecieGerencial")
    .rdd.flatMap(lambda x: x)
    .collect()
)


print("üîù ESP√âCIES TOP 80% PORTATEIS:")
print(especies_top80)
print(f"Total de esp√©cies: {len(especies_top80)}")

# SKUs das esp√©cies top 80%
skus_especies_top80 = (
    spark.table('data_engineering_prd.app_venda.mercadoria')
    .select(
        F.col("CdSkuLoja").alias("CdSku"),
        F.col("NmEspecieGerencial")
    )
    .filter(F.col("NmEspecieGerencial").isin(especies_top80))
    .filter(F.col("CdSku") != -1)
    .select("CdSku")
    .rdd.flatMap(lambda x: x)
    .collect()
)

print(f"\nüìä SKUs das esp√©cies top 80%: {len(skus_especies_top80)} SKUs")

# Valida√ß√£o dos percentuais
print("\nüìà VALIDA√á√ÉO PERCENTUAIS:")
print("ESP√âCIES TOP 80%:")
df_demanda_especie.filter(F.col("NmEspecieGerencial").isin(especies_top80)).agg(F.sum("PercDemanda")).show()

# COMMAND ----------

# Tabelas por categoria
TABELAS_MATRIZ_MERECIMENTO = {
    "DIRETORIA DE TELAS": {
        "offline": "databox.bcg_comum.supply_matriz_merecimento_de_telas_teste0710",
        "online": "databox.bcg_comum.supply_matriz_merecimento_de_telas_online_teste0710",
        "grupo_apelido": "telas"
    },
    # "DIRETORIA TELEFONIA CELULAR": {
    #     "offline": "databox.bcg_comum.supply_matriz_merecimento_telefonia_celular_teste0710",
    #     "online": "databox.bcg_comum.supply_matriz_merecimento_telefonia_celular_online_teste0710",
    #     "grupo_apelido": "telefonia"
    # },
    # "DIRETORIA LINHA LEVE": {
    #     "offline": "databox.bcg_comum.supply_matriz_merecimento_LINHA_LEVE_teste0410",
    #     "online": "databox.bcg_comum.supply_matriz_merecimento_LINHA_LEVE_online_teste0310",
    #     "grupo_apelido": "linha_leve"
    # },
}

# Pasta de sa√≠da
PASTA_OUTPUT = "/Workspace/Users/lucas.arodrigues-ext@viavarejo.com.br/usuarios/scardini/supply_matriz_de_merecimento/src/output"

# Colunas de merecimento por categoria
COLUNAS_MERECIMENTO = {
    "DIRETORIA DE TELAS": "Merecimento_Final_MediaAparada90_Qt_venda_sem_ruptura",
    "DIRETORIA TELEFONIA CELULAR": "Merecimento_Final_MediaAparada90_Qt_venda_sem_ruptura",
    "DIRETORIA LINHA LEVE": "Merecimento_Final_MediaAparada180_Qt_venda_sem_ruptura",
}

# Filtros
FILTROS_GRUPO_REMOCAO = {
    "DIRETORIA DE TELAS": ["FORA DE LINHA", 
                           "SEM_GN"
                            "TV 40 MEDIO P",
                            "TV 43 QLED ALTO",
                            "TV 50 ESP - QLED / MINI LED",
                            "TV 55 ESP MEDIO",
                            "TV 55 QLED / OLED ALTO",
                            "TV 55 QLED PP",
                            "TV 60 ALTO P",
                            "TV 65 MINI LED MEDIO",
                            "TV 65 NEO QLED ALTO",
                            "TV 65 QLED / OLED ALTO",
                            "TV 65 QLED / OLED PP",
                            "TV 65 QNED ALTO",
                            "TV 65 QNED MEDIO",
                            "TV 65 QNED PP",
                            "TV 70 ALTO P",
                            "TV 75 NEO QLED ALTO",
                            "TV 75 PP",
                            "TV 75 QLED / OLED ALTO",
                            "TV 75 QLED PP",
                            "TV 75 QNED ALTO",
                            "TV 75 QNED MEDIO",],
    
    "DIRETORIA TELEFONIA CELULAR": ["FORA DE LINHA", "SEM_GN", ">4000", "3001 a 3500"],
    "DIRETORIA LINHA LEVE": ["FORA DE LINHA", "SEM_GN", "ASPIRADOR DE PO_BIV"],
}

FLAG_SELECAO_REMOCAO = {
    "DIRETORIA DE TELAS": "REMO√á√ÉO",
    "DIRETORIA TELEFONIA CELULAR": "REMO√á√ÉO",
    "DIRETORIA LINHA LEVE": "REMO√á√ÉO",
}

FILTROS_GRUPO_SELECAO = {
    "DIRETORIA DE TELAS": [],
    "DIRETORIA TELEFONIA CELULAR": ["Telef pp"],
    "DIRETORIA LINHA LEVE": [],
}

# Limite de linhas por arquivo
MAX_LINHAS_POR_ARQUIVO = 150000

# Configura√ß√µes de filtros de produtos por categoria
FILTROS_PRODUTOS = {
    "DIRETORIA DE TELAS": {
        "tipificacao_entrega": ["SL"],  # Apenas SL (Sai Loja)
        "marcas_excluidas": [],  # Excluir marca APPLE
        "aplicar_filtro": True
    },
    "DIRETORIA TELEFONIA CELULAR": {
        "tipificacao_entrega": ["SL"],  # Apenas SL (Sai Loja)
        "marcas_excluidas": ["APPLE"],  # Excluir marca APPLE
        "aplicar_filtro": True
    },
    "DIRETORIA LINHA LEVE": {
        "tipificacao_entrega": ["SL"],  # Apenas SL (Sai Loja)
        "marcas_excluidas": ["APPLE"],  # Excluir marca APPLE
        "aplicar_filtro": True
    },
    "DIRETORIA LINHA BRANCA": {
        "tipificacao_entrega": ["SL"],  # Apenas SL (Sai Loja)
        "marcas_excluidas": ["APPLE"],  # Excluir marca APPLE
        "aplicar_filtro": True
    }
}

# Configura√ß√£o global de filtros de produtos (fallback)
FILTROS_PRODUTOS_GLOBAL = {
    "tipificacao_entrega": ["SL"],  # Apenas SL (Sai Loja)
    "marcas_excluidas": ["APPLE"],  # Excluir marca APPLE
    "aplicar_filtro": True
}

print("‚úÖ Configura√ß√µes carregadas")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Fun√ß√µes de Formata√ß√£o

# COMMAND ----------

def formatar_codigo_loja(cdfilial: int, is_cd: bool) -> str:
    """
    Formata c√≥digo da loja/CD no padr√£o 0021_0XXXX ou 0099_0XXXX.
    
    Regras:
    - Loja (is_cd=False): 0021_0XXXX (5 d√≠gitos com zeros √† esquerda)
    - CD/Entreposto (is_cd=True): 0099_0XXXX (5 d√≠gitos com zeros √† esquerda)
    
    Exemplos:
    - formatar_codigo_loja(1234, False) ‚Üí "0021_01234" (loja)
    - formatar_codigo_loja(7, False) ‚Üí "0021_00007" (loja)
    - formatar_codigo_loja(1401, True) ‚Üí "0099_01401" (CD)
    - formatar_codigo_loja(1501, True) ‚Üí "0099_01501" (Entreposto)
    """
    prefixo = "0099" if is_cd else "0021"
    return f"{prefixo}_{cdfilial:05d}"

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Fun√ß√µes de Processamento

# COMMAND ----------

def diagnosticar_diferenca_canais(df_offline: DataFrame, df_online: DataFrame, categoria: str) -> None:
    """
    Diagn√≥stico de diferen√ßas entre canais OFFLINE e ONLINE.
    
    Investiga granularidade, SKUs √∫nicos, grupos de necessidade e filiais
    para identificar por que h√° diferen√ßas significativas de volume.
    
    Args:
        df_offline: DataFrame do canal offline
        df_online: DataFrame do canal online
        categoria: Nome da categoria
    """
    print("\n" + "="*80)
    print(f"üîç DIAGN√ìSTICO COMPARATIVO - {categoria}")
    print("="*80)
    
    # 1. Contagens b√°sicas
    count_offline = df_offline.count()
    count_online = df_online.count()
    ratio = count_online / count_offline if count_offline > 0 else 0
    
    print(f"\nüìä VOLUMES TOTAIS:")
    print(f"  ‚Ä¢ OFFLINE: {count_offline:,} registros")
    print(f"  ‚Ä¢ ONLINE:  {count_online:,} registros")
    print(f"  ‚Ä¢ Raz√£o:   {ratio:.1f}x {'üö® MUITO ALTO' if ratio > 5 else '‚ö†Ô∏è  ALTO' if ratio > 2 else '‚úÖ OK'}")
    
    # 2. SKUs √∫nicos - AN√ÅLISE DETALHADA
    print(f"\nüè∑Ô∏è  AN√ÅLISE DE SKUs:")
    
    skus_offline_set = set([row.CdSku for row in df_offline.select("CdSku").distinct().collect()])
    skus_online_set = set([row.CdSku for row in df_online.select("CdSku").distinct().collect()])
    
    skus_apenas_offline = skus_offline_set - skus_online_set
    skus_apenas_online = skus_online_set - skus_offline_set
    skus_em_ambos = skus_offline_set & skus_online_set
    
    print(f"  ‚Ä¢ SKUs OFFLINE: {len(skus_offline_set):,} SKUs √∫nicos")
    print(f"  ‚Ä¢ SKUs ONLINE:  {len(skus_online_set):,} SKUs √∫nicos")
    print(f"  ‚Ä¢ SKUs em AMBOS: {len(skus_em_ambos):,} SKUs")
    print(f"  ‚Ä¢ SKUs APENAS OFFLINE: {len(skus_apenas_offline):,} SKUs")
    print(f"  ‚Ä¢ SKUs APENAS ONLINE:  {len(skus_apenas_online):,} SKUs")
    
    if len(skus_apenas_online) > 0:
        print(f"  üí° ONLINE tem {len(skus_apenas_online):,} SKUs exclusivos")
    if len(skus_apenas_offline) > 0:
        print(f"  ‚ö†Ô∏è  OFFLINE tem {len(skus_apenas_offline):,} SKUs que n√£o aparecem no ONLINE")
    
    # Valida√ß√£o do filtro TOP 80% para Linha Leve
    if categoria == "DIRETORIA LINHA LEVE":
        print(f"\nüîù VALIDA√á√ÉO FILTRO TOP 80% ESP√âCIES:")
        print(f"  ‚Ä¢ Esp√©cies top 80% definidas: {len(especies_top80)}")
        print(f"  ‚Ä¢ SKUs das esp√©cies: {len(skus_especies_top80)}")
        
        skus_top80_em_offline = len(skus_offline_set & set(skus_especies_top80))
        skus_top80_em_online = len(skus_online_set & set(skus_especies_top80))
        
        print(f"  ‚Ä¢ SKUs top 80% presentes no OFFLINE: {skus_top80_em_offline:,} ({skus_top80_em_offline/len(skus_especies_top80)*100:.1f}%)")
        print(f"  ‚Ä¢ SKUs top 80% presentes no ONLINE:  {skus_top80_em_online:,} ({skus_top80_em_online/len(skus_especies_top80)*100:.1f}%)")
        
        if skus_top80_em_offline < len(skus_especies_top80):
            print(f"  ‚ö†Ô∏è  {len(skus_especies_top80) - skus_top80_em_offline} SKUs top 80% ausentes no OFFLINE")
        if skus_top80_em_online < len(skus_especies_top80):
            print(f"  ‚ö†Ô∏è  {len(skus_especies_top80) - skus_top80_em_online} SKUs top 80% ausentes no ONLINE")
    
    # 3. Filiais √∫nicas
    filiais_offline = df_offline.select("CdFilial").distinct().count()
    filiais_online = df_online.select("CdFilial").distinct().count()
    
    print(f"\nüè™ FILIAIS √öNICAS:")
    print(f"  ‚Ä¢ OFFLINE: {filiais_offline:,} filiais")
    print(f"  ‚Ä¢ ONLINE:  {filiais_online:,} filiais")
    print(f"  ‚Ä¢ Diferen√ßa: {filiais_online - filiais_offline:+,} filiais (+{(filiais_online/filiais_offline - 1)*100:.1f}%)")
    
    # 4. Granularidade m√©dia (registros por filial e por SKU)
    registros_por_filial_offline = count_offline / filiais_offline if filiais_offline > 0 else 0
    registros_por_filial_online = count_online / filiais_online if filiais_online > 0 else 0
    
    registros_por_sku_offline = count_offline / len(skus_offline_set) if len(skus_offline_set) > 0 else 0
    registros_por_sku_online = count_online / len(skus_online_set) if len(skus_online_set) > 0 else 0
    
    print(f"\nüìè GRANULARIDADE:")
    print(f"  ‚Ä¢ OFFLINE: {registros_por_filial_offline:.1f} registros/filial | {registros_por_sku_offline:.1f} registros/SKU")
    print(f"  ‚Ä¢ ONLINE:  {registros_por_filial_online:.1f} registros/filial | {registros_por_sku_online:.1f} registros/SKU")
    print(f"  ‚Ä¢ Raz√£o registros/filial: {registros_por_filial_online / registros_por_filial_offline:.1f}x")
    
    # 5. An√°lise de causa prov√°vel
    print(f"\nüîé DIAGN√ìSTICO FINAL:")
    if ratio > 5:
        print(f"  üö® ALERTA CR√çTICO: Diferen√ßa de {ratio:.1f}x √© EXTREMAMENTE ALTA")
        print(f"  ")
        print(f"  üìå Causas identificadas:")
        if len(skus_apenas_online) > len(skus_online_set) * 0.3:
            print(f"     ‚Ä¢ ONLINE tem {len(skus_apenas_online):,} SKUs exclusivos ({len(skus_apenas_online)/len(skus_online_set)*100:.1f}% do total)")
        if registros_por_filial_online > registros_por_filial_offline * 3:
            print(f"     ‚Ä¢ Granularidade {registros_por_filial_online / registros_por_filial_offline:.1f}x maior no ONLINE")
            print(f"       (Possivelmente desagregado por SKU individual vs agregado por grupo)")
        if filiais_online > filiais_offline * 1.2:
            print(f"     ‚Ä¢ ONLINE tem {filiais_online - filiais_offline:,} filiais a mais ({(filiais_online/filiais_offline - 1)*100:.1f}%)")
    elif ratio > 2:
        print(f"  ‚ö†Ô∏è  Diferen√ßa de {ratio:.1f}x √© ALTA mas pode ser aceit√°vel")
        print(f"  üí° Causas: ONLINE tem mais filiais ({filiais_online - filiais_offline:+,}) e/ou mais SKUs ({len(skus_online_set) - len(skus_offline_set):+,})")
    else:
        print(f"  ‚úÖ Diferen√ßa de {ratio:.1f}x est√° dentro do esperado para opera√ß√µes Online/Offline")
    
    print("="*80 + "\n")

# COMMAND ----------

def carregar_e_filtrar_matriz(categoria: str, canal: str) -> DataFrame:
    """
    Carrega matriz de merecimento e aplica filtros.
    
    Args:
        categoria: Nome da categoria
        canal: "offline" ou "online"
        
    Returns:
        DataFrame com CdSku, CdFilial, Merecimento_raw
    """
    print(f"\nüîÑ Carregando matriz: {categoria} - {canal.upper()}")
    print("-" * 80)
    
    tabela = TABELAS_MATRIZ_MERECIMENTO[categoria][canal]
    coluna_merecimento = COLUNAS_MERECIMENTO[categoria]
    flag_tipo = FLAG_SELECAO_REMOCAO[categoria]
    filtros_remocao = FILTROS_GRUPO_REMOCAO[categoria]
    filtros_selecao = FILTROS_GRUPO_SELECAO[categoria]
    
    # Carregar dados base
    df_base = (
        spark.table(tabela)
        .select(
            "CdFilial", "CdSku", "grupo_de_necessidade",
            (100 * F.col(coluna_merecimento)).alias("Merecimento_raw")
        )
    )
    
    # CHECKPOINT 1: Dados brutos
    skus_inicial = df_base.select("CdSku").distinct().count()
    registros_inicial = df_base.count()
    print(f"üì¶ DADOS BRUTOS DA TABELA:")
    print(f"  ‚Ä¢ Registros: {registros_inicial:,}")
    print(f"  ‚Ä¢ SKUs √∫nicos: {skus_inicial:,}")
    
    # FILTRO DE PRODUTOS: Configur√°vel por categoria
    filtros_produtos = FILTROS_PRODUTOS.get(categoria, FILTROS_PRODUTOS_GLOBAL)
    
    if filtros_produtos.get("aplicar_filtro", False):
        print(f"\nüè∑Ô∏è FILTRO DE PRODUTOS:")
        print(f"  ‚Ä¢ Incluir apenas: {filtros_produtos['tipificacao_entrega']}")
        print(f"  ‚Ä¢ Excluir marcas: {filtros_produtos['marcas_excluidas']}")
        
        # Carregar informa√ß√µes de produtos da tabela mercadoria
        df_mercadoria = (
            spark.table('data_engineering_prd.app_venda.mercadoria')
            .select(
                F.col("CdSkuLoja").alias("CdSku"),
                "StTipificacaoEntrega", 
                "NmMarca"
            )
            .distinct()
        )
        
        # Log inicial da tabela mercadoria
        total_produtos_inicial = df_mercadoria.count()
        print(f"  üìä Produtos na tabela mercadoria: {total_produtos_inicial:,}")
        
        # Mostrar distribui√ß√£o por tipifica√ß√£o de entrega
        print(f"  üìã Tipifica√ß√µes de entrega dispon√≠veis:")
        tipificacoes_disponiveis = (
            df_mercadoria
            .groupBy("StTipificacaoEntrega")
            .count()
            .orderBy(F.desc("count"))
        )
        tipificacoes_disponiveis.show(10, truncate=False)
        
        # Aplicar filtros de produto
        df_produtos_filtrados = df_mercadoria
        
        # Filtro por tipifica√ß√£o de entrega
        if filtros_produtos["tipificacao_entrega"]:
            produtos_antes_tipificacao = df_produtos_filtrados.count()
            df_produtos_filtrados = df_produtos_filtrados.filter(
                F.col("StTipificacaoEntrega").isin(filtros_produtos["tipificacao_entrega"])
            )
            produtos_apos_tipificacao = df_produtos_filtrados.count()
            print(f"  ‚úÖ Filtro tipifica√ß√£o: {produtos_antes_tipificacao:,} ‚Üí {produtos_apos_tipificacao:,} (-{produtos_antes_tipificacao - produtos_apos_tipificacao:,})")
            
            # Verificar se filtro funcionou
            tipificacoes_restantes = (
                df_produtos_filtrados
                .select("StTipificacaoEntrega")
                .distinct()
                .rdd.flatMap(lambda x: x)
                .collect()
            )
            print(f"  üîç Tipifica√ß√µes restantes: {sorted(tipificacoes_restantes)}")
        
        # Filtro por marcas exclu√≠das
        if filtros_produtos["marcas_excluidas"]:
            produtos_antes_marca = df_produtos_filtrados.count()
            df_produtos_filtrados = df_produtos_filtrados.filter(
                ~F.col("NmMarca").isin(filtros_produtos["marcas_excluidas"])
            )
            produtos_apos_marca = df_produtos_filtrados.count()
            print(f"  ‚úÖ Filtro marcas: {produtos_antes_marca:,} ‚Üí {produtos_apos_marca:,} (-{produtos_antes_marca - produtos_apos_marca:,})")
            
            # Verificar se marcas exclu√≠das foram removidas
            marcas_restantes = (
                df_produtos_filtrados
                .select("NmMarca")
                .distinct()
                .rdd.flatMap(lambda x: x)
                .collect()
            )
            marcas_excluidas_encontradas = [m for m in filtros_produtos["marcas_excluidas"] if m in marcas_restantes]
            if marcas_excluidas_encontradas:
                print(f"  ‚ö†Ô∏è ATEN√á√ÉO: Marcas que deveriam ser exclu√≠das ainda est√£o presentes: {marcas_excluidas_encontradas}")
            else:
                print(f"  ‚úÖ Marcas exclu√≠das removidas com sucesso: {filtros_produtos['marcas_excluidas']}")
        
        # Log final dos produtos filtrados
        total_produtos_final = df_produtos_filtrados.count()
        print(f"  üìä Produtos ap√≥s filtros: {total_produtos_final:,} (-{total_produtos_inicial - total_produtos_final:,})")
    else:
        print(f"\nüè∑Ô∏è FILTRO DE PRODUTOS:")
        print(f"  ‚Ä¢ Filtro desabilitado para {categoria}")
        df_produtos_filtrados = None
    
    # Aplicar filtro de produtos se habilitado
    if df_produtos_filtrados is not None:
        # Fazer join com dados base para aplicar filtro
        df_base_filtrado = (
            df_base
            .join(df_produtos_filtrados, on="CdSku", how="inner")
            .select("CdFilial", "CdSku", "grupo_de_necessidade", "Merecimento_raw")
        )
        
        # CHECKPOINT 2: Ap√≥s filtro de produtos
        skus_pos_produto = df_base_filtrado.select("CdSku").distinct().count()
        registros_pos_produto = df_base_filtrado.count()
        print(f"  ‚Ä¢ SKUs ap√≥s filtro de produtos: {skus_pos_produto:,} ({skus_pos_produto - skus_inicial:+,})")
        print(f"  ‚Ä¢ Registros ap√≥s filtro de produtos: {registros_pos_produto:,} ({registros_pos_produto - registros_inicial:+,})")
        
        # Usar dados filtrados como base para pr√≥ximos filtros
        df_base = df_base_filtrado
    else:
        print(f"  ‚Ä¢ Filtro de produtos n√£o aplicado - usando dados originais")
    
    # Mostrar grupos dispon√≠veis antes do filtro
    grupos_disponiveis = df_base.select("grupo_de_necessidade").distinct().rdd.flatMap(lambda x: x).collect()
    print(f"\nüìã GRUPOS DISPON√çVEIS:")
    print(f"  ‚Ä¢ Total: {len(grupos_disponiveis)} grupos")
    print(f"  ‚Ä¢ Lista: {sorted(grupos_disponiveis)}")
    
    # Aplicar filtros de grupo
    print(f"\nüéØ FILTRO DE GRUPOS DE NECESSIDADE:")
    if flag_tipo == "SELE√á√ÉO":
        df_filtrado = df_base.filter(F.col("grupo_de_necessidade").isin(filtros_selecao))
        print(f"  ‚Ä¢ Tipo: SELE√á√ÉO")
        print(f"  ‚Ä¢ Grupos selecionados: {len(filtros_selecao)}")
        print(f"  ‚Ä¢ Grupos solicitados: {filtros_selecao}")
    else:
        df_filtrado = df_base.filter(~F.col("grupo_de_necessidade").isin(filtros_remocao))
        print(f"  ‚Ä¢ Tipo: REMO√á√ÉO")
        print(f"  ‚Ä¢ Grupos removidos: {len(filtros_remocao)}")
        print(f"  ‚Ä¢ Grupos solicitados para remo√ß√£o: {filtros_remocao}")
        
        # Verificar quais grupos solicitados realmente existem
        grupos_existentes = [g for g in filtros_remocao if g in grupos_disponiveis]
        grupos_inexistentes = [g for g in filtros_remocao if g not in grupos_disponiveis]
        
        if grupos_inexistentes:
            print(f"  ‚ö†Ô∏è Grupos solicitados mas N√ÉO EXISTENTES: {grupos_inexistentes}")
        if grupos_existentes:
            print(f"  ‚úÖ Grupos que SER√ÉO removidos: {grupos_existentes}")
    
    # CHECKPOINT 2: Ap√≥s filtro de grupos
    skus_pos_grupo = df_filtrado.select("CdSku").distinct().count()
    registros_pos_grupo = df_filtrado.count()
    print(f"  ‚Ä¢ SKUs ap√≥s filtro: {skus_pos_grupo:,} ({skus_pos_grupo - skus_inicial:+,})")
    print(f"  ‚Ä¢ Registros ap√≥s filtro: {registros_pos_grupo:,} ({registros_pos_grupo - registros_inicial:+,})")
    
    # Verificar grupos restantes ap√≥s filtro
    grupos_restantes = df_filtrado.select("grupo_de_necessidade").distinct().rdd.flatMap(lambda x: x).collect()
    print(f"  ‚Ä¢ Grupos restantes ap√≥s filtro: {len(grupos_restantes)}")
    print(f"  ‚Ä¢ Lista dos grupos restantes: {sorted(grupos_restantes)}")
    
    # Verificar se grupos solicitados para remo√ß√£o ainda est√£o presentes
    grupos_nao_removidos = [g for g in filtros_remocao if g in grupos_restantes]
    if grupos_nao_removidos:
        print(f"  ‚ùå ERRO: Grupos solicitados para remo√ß√£o ainda est√£o presentes: {grupos_nao_removidos}")
        for grupo in grupos_nao_removidos:
            registros_grupo = df_filtrado.filter(F.col("grupo_de_necessidade") == grupo).count()
            print(f"    ‚Ä¢ {grupo}: {registros_grupo:,} registros")
        print(f"  ‚ö†Ô∏è CORRE√á√ÉO: Aplicando filtro adicional para remover grupos restantes...")
        df_filtrado = df_filtrado.filter(~F.col("grupo_de_necessidade").isin(grupos_nao_removidos))
        print(f"  ‚úÖ Grupos removidos com filtro adicional")
    else:
        print(f"  ‚úÖ Todos os grupos solicitados para remo√ß√£o foram removidos com sucesso")
    
    # Filtro especial para Linha Leve: apenas SKUs das esp√©cies top 80% de PORTATEIS
    if categoria == "DIRETORIA LINHA LEVE":
        print(f"\nüîù FILTRO TOP 80% ESP√âCIES PORTATEIS:")
        print(f"  ‚Ä¢ Esp√©cies top 80% definidas: {len(especies_top80)}")
        print(f"  ‚Ä¢ SKUs das esp√©cies: {len(skus_especies_top80)}")
        
        skus_antes_top80 = df_filtrado.select("CdSku").distinct().count()
        df_filtrado = df_filtrado.filter(F.col("CdSku").isin(skus_especies_top80))
        skus_apos_top80 = df_filtrado.select("CdSku").distinct().count()
        registros_apos_top80 = df_filtrado.count()
        
        print(f"  ‚Ä¢ SKUs antes: {skus_antes_top80:,}")
        print(f"  ‚Ä¢ SKUs ap√≥s: {skus_apos_top80:,} ({skus_apos_top80 - skus_antes_top80:+,})")
        print(f"  ‚Ä¢ Registros ap√≥s: {registros_apos_top80:,}")
        
        if skus_apos_top80 != len(skus_especies_top80):
            print(f"  ‚ö†Ô∏è  ATEN√á√ÉO: {len(skus_especies_top80) - skus_apos_top80} SKUs top 80% n√£o encontrados nos dados!")
    
    # Regra especial online: CdFilial 1401 ‚Üí 14 (apenas para TELAS e TELEFONIA)
    if canal == "online" and categoria in ["DIRETORIA DE TELAS", "DIRETORIA TELEFONIA CELULAR"]:
        print(f"\nüîÑ CONSOLIDA√á√ÉO DE FILIAIS:")
        filial_1401_count = df_filtrado.filter(F.col("CdFilial") == 1401).count()
        
        # Aplicar consolida√ß√£o 1401 ‚Üí 14
        df_filtrado = df_filtrado.withColumn(
            "CdFilial", 
            F.when(F.col("CdFilial") == 1401, 14).otherwise(F.col("CdFilial"))
        )
        
        print(f"  ‚Ä¢ CdFilial 1401 ‚Üí 14 (apenas {categoria})")
        print(f"  ‚Ä¢ Registros consolidados: {filial_1401_count:,}")
        
        # Agregar por CdSku + CdFilial + grupo_de_necessidade para somar merecimentos
        print(f"  ‚Ä¢ Agregando merecimentos ap√≥s consolida√ß√£o...")
        df_filtrado = (
            df_filtrado
            .groupBy("CdSku", "CdFilial", "grupo_de_necessidade")
            .agg(F.sum("Merecimento_raw").alias("Merecimento_raw"))
        )
    
    # Agregar por CdSku + CdFilial (agrega√ß√£o final)
    print(f"\nüìä AGREGA√á√ÉO FINAL:")
    df_agregado = (
        df_filtrado
        .groupBy("CdSku", "CdFilial")
        .agg(F.avg("Merecimento_raw").alias("Merecimento"))
        .withColumn("CANAL", F.lit(canal.upper()))
    )
    
    registros_final = df_agregado.count()
    skus_final = df_agregado.select("CdSku").distinct().count()
    filiais_final = df_agregado.select("CdFilial").distinct().count()
    
    print(f"  ‚Ä¢ Registros finais: {registros_final:,}")
    print(f"  ‚Ä¢ SKUs finais: {skus_final:,}")
    print(f"  ‚Ä¢ Filiais finais: {filiais_final:,}")
    print(f"  ‚Ä¢ Granularidade: {registros_final / filiais_final:.1f} registros/filial")
    print("-" * 80)
    print(f"‚úÖ Carregamento conclu√≠do: {canal.upper()}")
    
    return df_agregado

# COMMAND ----------

def normalizar_para_100_exato(df: DataFrame) -> DataFrame:
    """
    Normaliza merecimentos para somar EXATAMENTE 100.00 por CdSku + CANAL.
    Ajusta diferen√ßa no maior merecimento de cada grupo.
    
    Processo:
    1. Proporcionalizar para ~100%
    2. Calcular diferen√ßa real vs 100.00
    3. Adicionar diferen√ßa no maior merecimento
    
    Args:
        df: DataFrame com CdSku, CdFilial, Merecimento, CANAL
        
    Returns:
        DataFrame com PERCENTUAL normalizado para 100.00 exato
    """
    print("üîÑ Normalizando para 100.00% exato...")
    
    # Janela por CdSku + CANAL
    window_sku_canal = W.partitionBy("CdSku", "CANAL")
    
    # 1. Proporcionalizar
    df_proporcional = (
        df
        .withColumn("soma_sku_canal", F.sum("Merecimento").over(window_sku_canal))
        .withColumn(
            "Merecimento_proporcional",
            F.when(F.col("soma_sku_canal") > 0, 
                   (F.col("Merecimento") / F.col("soma_sku_canal")) * 100.0)
            .otherwise(0.0)
        )
    )
    
    # 2. Identificar maior merecimento por CdSku + CANAL
    window_rank = W.partitionBy("CdSku", "CANAL").orderBy(F.desc("Merecimento_proporcional"))
    
    df_com_rank = (
        df_proporcional
        .withColumn("rank", F.row_number().over(window_rank))
    )
    
    # 3. Calcular diferen√ßa para 100.00
    df_com_diferenca = (
        df_com_rank
        .withColumn("soma_proporcional", F.sum("Merecimento_proporcional").over(window_sku_canal))
        .withColumn("diferenca_100", 100.0 - F.col("soma_proporcional"))
    )
    
    # 4. Ajustar apenas o maior merecimento (rank = 1) para soma exata 100.000
    df_ajustado = (
        df_com_diferenca
        .withColumn(
            "PERCENTUAL",
            F.when(F.col("rank") == 1, 
                   F.col("Merecimento_proporcional") + F.col("diferenca_100"))
            .otherwise(F.col("Merecimento_proporcional"))
        )
        .withColumn("PERCENTUAL", F.round(F.col("PERCENTUAL"), 3))
        .select("CdSku", "CdFilial", "CANAL", "LOJA", "PERCENTUAL")
    )
    
    # Valida√ß√£o
    soma_validacao = (
        df_ajustado
        .groupBy("CdSku", "CANAL")
        .agg(F.sum("PERCENTUAL").alias("soma_total"))
    )
    
    nao_100 = soma_validacao.filter((F.col("soma_total") < 99.9999) | (F.col("soma_total") > 100.0001)).count()
    
    if nao_100 > 0:
        print(f"  ‚ö†Ô∏è ATEN√á√ÉO: {nao_100} grupos n√£o somam exatamente 100.000%")
        soma_validacao.filter((F.col("soma_total") < 99.9999) | (F.col("soma_total") > 100.0001)).show(5, truncate=False)
    else:
        print(f"  ‚úÖ Todos os grupos somam exatamente 100.000%")
    
    # 5. Corre√ß√£o final para garantir exatid√£o matem√°tica
    print("  üîß Aplicando corre√ß√£o final para exatid√£o matem√°tica...")
    df_final_corrigido = garantir_soma_exata_100(df_ajustado)
    
    print(f"‚úÖ Normaliza√ß√£o conclu√≠da: {df_final_corrigido.count():,} registros")
    
    return df_final_corrigido

def garantir_soma_exata_100(df: DataFrame) -> DataFrame:
    """
    Garante que todas as somas por SKU+CANAL sejam exatamente 100.000%.
    Aplica corre√ß√£o final no maior merecimento de cada grupo.
    """
    print("    üîß Garantindo soma exata de 100.000%...")
    
    # Window para agrupar por SKU+CANAL
    window_sku_canal = W.partitionBy("CdSku", "CANAL")
    window_rank = W.partitionBy("CdSku", "CANAL").orderBy(F.desc("PERCENTUAL"))
    
    # Calcular soma atual e diferen√ßa exata
    df_com_soma = (
        df
        .withColumn("soma_atual", F.sum("PERCENTUAL").over(window_sku_canal))
        .withColumn("diferenca_exata", 100.0 - F.col("soma_atual"))
    )
    
    # Aplicar corre√ß√£o no maior merecimento de cada grupo
    df_com_rank = (
        df_com_soma
        .withColumn("rank", F.row_number().over(window_rank))
    )
    
    df_corrigido = (
        df_com_rank
        .withColumn(
            "PERCENTUAL",
            F.when(F.col("rank") == 1, 
                   F.col("PERCENTUAL") + F.col("diferenca_exata"))
            .otherwise(F.col("PERCENTUAL"))
        )
        .withColumn("PERCENTUAL", F.round(F.col("PERCENTUAL"), 3))
        .select("CdSku", "CdFilial", "CANAL", "LOJA", "PERCENTUAL")
    )
    
    # Valida√ß√£o final rigorosa
    validacao_final = (
        df_corrigido
        .groupBy("CdSku", "CANAL")
        .agg(F.sum("PERCENTUAL").alias("soma_final"))
    )
    
    nao_100_final = validacao_final.filter(F.abs(F.col("soma_final") - 100.0) > 0.0001).count()
    
    if nao_100_final > 0:
        print(f"    ‚ùå ERRO: {nao_100_final} grupos ainda n√£o somam exatamente 100.000%")
        validacao_final.filter(F.abs(F.col("soma_final") - 100.0) > 0.0001).show(3, truncate=False)
        raise ValueError(f"{nao_100_final} grupos n√£o somam exatamente 100.000% ap√≥s corre√ß√£o")
    else:
        print(f"    ‚úÖ Todos os grupos somam exatamente 100.000%")
    
    return df_corrigido

# COMMAND ----------

def adicionar_informacoes_filial(df: DataFrame) -> DataFrame:
    """
    Adiciona informa√ß√µes de filiais e cria coluna LOJA formatada.
    
    Nova l√≥gica de preserva√ß√£o:
    1. CD 14: SEMPRE PRESERVAR (consolidado)
    2. CDs ativos: PRESERVAR
    3. Lojas ativas: PRESERVAR
    4. Outros: REMOVER com relat√≥rio detalhado
    
    Args:
        df: DataFrame com CdFilial, CANAL
        
    Returns:
        DataFrame com coluna LOJA adicionada e filiais n√£o eleg√≠veis removidas
    """
    print("üîÑ Adicionando informa√ß√µes de filiais...")
    
    # Contar registros antes
    registros_antes = df.count()
    print(f"  üìä Registros antes do filtro: {registros_antes:,}")
    
    # Carregar tabelas de refer√™ncia
    print("  üìã Carregando tabelas de refer√™ncia...")
    
    # CDs ativos
    df_cds = (
        spark.table('databox.logistica_comum.roteirizacaocentrodistribuicao')
        .select("CdFilial", "NmFilial", "NmTipoFilial")
        .withColumn("tipo_filial", F.col("NmTipoFilial"))
    )
    
    # Lojas ativas
    df_lojas = (
        spark.table('data_engineering_prd.app_operacoesloja.roteirizacaolojaativa')
        .select("CdFilial", "NmFilial")
        .withColumn("NmTipoFilial", F.lit(None).cast("string"))
        .withColumn("tipo_filial", F.lit("LOJA"))
    )
    
    print(f"    ‚Ä¢ CDs ativos: {df_cds.count():,}")
    print(f"    ‚Ä¢ Lojas ativas: {df_lojas.count():,}")
    
    # Unir tabelas de refer√™ncia
    df_referencia = df_cds.union(df_lojas)
    
    # Identificar filiais eleg√≠veis (CD 14 + CDs ativos + lojas ativas)
    df_com_status = (
        df
        .join(df_referencia, on="CdFilial", how="left")
        .withColumn(
            "elegivel",
            F.when(F.col("CdFilial") == 14, F.lit(True))  # CD 14 sempre eleg√≠vel
            .when(F.col("NmFilial").isNotNull(), F.lit(True))  # Est√° na refer√™ncia
            .otherwise(F.lit(False))
        )
    )
    
    # Separar eleg√≠veis e n√£o eleg√≠veis
    df_elegiveis = df_com_status.filter(F.col("elegivel") == True)
    df_removidos = df_com_status.filter(F.col("elegivel") == False)
    
    registros_elegiveis = df_elegiveis.count()
    registros_removidos = df_removidos.count()
    
    print(f"  ‚úÖ Filiais eleg√≠veis: {registros_elegiveis:,} registros")
    print(f"  ‚ùå Filiais removidas: {registros_removidos:,} registros")
    
    # Relat√≥rio detalhado dos removidos
    if registros_removidos > 0:
        print(f"\n  üìã RELAT√ìRIO DE FILIAIS REMOVIDAS:")
        
        # Filiais removidas
        filiais_removidas = (
            df_removidos
            .select("CdFilial")
            .distinct()
            .orderBy("CdFilial")
        )
        print(f"    ‚Ä¢ Filiais removidas: {filiais_removidas.count()}")
        print("    ‚Ä¢ Lista das filiais:")
        filiais_removidas.show(20, truncate=False)
        
        # SKUs com maior merecimento nos removidos
        skus_removidos = (
            df_removidos
            .groupBy("CdSku")
            .agg(F.sum("Merecimento").alias("total_merecimento"))
            .orderBy(F.desc("total_merecimento"))
            .limit(10)
        )
        print(f"    ‚Ä¢ Top 10 SKUs removidos:")
        skus_removidos.show(10, truncate=False)
    
    # Processar apenas os eleg√≠veis
    df_com_tipo = (
        df_elegiveis
        .withColumn(
            "is_cd",
            F.when(F.col("CdFilial") == 14, F.lit(True))  # CD 14 √© CD
            .when(F.col("tipo_filial").isin(["CD", "Entreposto", "TERMINAL"]), F.lit(True))
            .otherwise(F.lit(False))
        )
    )
    
    # UDF para formatar loja
    from pyspark.sql.types import StringType
    formatar_loja_udf = F.udf(
        lambda cdfilial, is_cd: formatar_codigo_loja(int(cdfilial), bool(is_cd)),
        StringType()
    )
    
    df_com_loja = (
        df_com_tipo
        .withColumn("LOJA", formatar_loja_udf(F.col("CdFilial"), F.col("is_cd")))
        .drop("is_cd", "NmFilial", "NmTipoFilial", "tipo_filial", "elegivel")
    )
    
    print(f"‚úÖ Informa√ß√µes adicionadas: {df_com_loja.count():,} registros")
    
    return df_com_loja

# COMMAND ----------

def criar_dataframe_final(df: DataFrame) -> DataFrame:
    """
    Cria DataFrame final com todas as colunas no formato do sistema.
    
    Colunas finais: SKU, CANAL, LOJA, DATA FIM, PERCENTUAL
    
    Args:
        df: DataFrame com CdSku, CANAL, LOJA, PERCENTUAL
        
    Returns:
        DataFrame formatado
    """
    print("üîÑ Criando DataFrame final...")
    
    df_final = (
        df
        .withColumn("SKU", F.col("CdSku").cast("string"))
        .withColumn("DATA FIM", F.lit(DATA_FIM_INT))
        .withColumn("PERCENTUAL", F.round(F.col("PERCENTUAL"), 3).cast("double"))
        .select("SKU", "CANAL", "LOJA", "DATA FIM", "PERCENTUAL")
        .orderBy("SKU", "LOJA", "CANAL")
    )
    
    print(f"‚úÖ DataFrame final criado: {df_final.count():,} registros")
    
    return df_final

# COMMAND ----------

def validar_integridade_dados_com_filtros(df: DataFrame, categoria: str) -> bool:
    """
    Valida integridade dos dados aplicando os mesmos filtros da exporta√ß√£o.
    
    Aplica os mesmos filtros de produtos que s√£o usados na exporta√ß√£o para garantir
    que estamos validando exatamente o que ser√° gerado.
    
    Args:
        df: DataFrame para valida√ß√£o
        categoria: Categoria sendo processada
        
    Returns:
        True se todas as valida√ß√µes passaram
    """
    print("üîç Validando integridade dos dados com filtros aplicados...")
    
    # Aplicar os mesmos filtros de produtos da exporta√ß√£o
    filtros_produtos = FILTROS_PRODUTOS.get(categoria, FILTROS_PRODUTOS_GLOBAL)
    
    if filtros_produtos.get("aplicar_filtro", False):
        print(f"  üè∑Ô∏è Aplicando filtros de produtos para valida√ß√£o:")
        print(f"    ‚Ä¢ Incluir apenas: {filtros_produtos['tipificacao_entrega']}")
        print(f"    ‚Ä¢ Excluir marcas: {filtros_produtos['marcas_excluidas']}")
        
        # Carregar informa√ß√µes de produtos da tabela mercadoria
        df_mercadoria = (
            spark.table('data_engineering_prd.app_venda.mercadoria')
            .select(
                F.col("CdSkuLoja").alias("CdSku"),
                "StTipificacaoEntrega", 
                "NmMarca"
            )
            .distinct()
        )
        
        # Aplicar filtros de produto
        df_produtos_filtrados = df_mercadoria
        
        # Filtro por tipifica√ß√£o de entrega
        if filtros_produtos["tipificacao_entrega"]:
            df_produtos_filtrados = df_produtos_filtrados.filter(
                F.col("StTipificacaoEntrega").isin(filtros_produtos["tipificacao_entrega"])
            )
        
        # Filtro por marcas exclu√≠das
        if filtros_produtos["marcas_excluidas"]:
            df_produtos_filtrados = df_produtos_filtrados.filter(
                ~F.col("NmMarca").isin(filtros_produtos["marcas_excluidas"])
            )
        
        # Aplicar filtro ao DataFrame de valida√ß√£o
        df_filtrado = (
            df
            .join(df_produtos_filtrados, df.SKU == df_produtos_filtrados.CdSku, how="inner")
            .select("SKU", "CANAL", "LOJA", "PERCENTUAL")
        )
        
        registros_antes = df.count()
        registros_apos = df_filtrado.count()
        print(f"    ‚Ä¢ Registros antes do filtro: {registros_antes:,}")
        print(f"    ‚Ä¢ Registros ap√≥s filtro: {registros_apos:,} (-{registros_antes - registros_apos:,})")
        
        # Usar DataFrame filtrado para valida√ß√£o
        df_validacao = df_filtrado
    else:
        print(f"  üè∑Ô∏è Filtros de produtos desabilitados para {categoria}")
        df_validacao = df
    
    # Chamar valida√ß√£o original com DataFrame filtrado
    return validar_integridade_dados(df_validacao)

def validar_integridade_dados(df: DataFrame) -> bool:
    """
    Valida integridade dos dados antes de dividir em arquivos.
    
    Valida√ß√µes:
    1. Somas por SKU+CANAL = 100%
    2. Chaves SKU-LOJA-CANAL aparecem uma √∫nica vez
    3. Para cada SKU, ambos os canais est√£o presentes (em pelo menos uma LOJA)
    
    Args:
        df: DataFrame para valida√ß√£o
        
    Returns:
        True se todas as valida√ß√µes passaram
    """
    print("üîç Validando integridade dos dados...")
    
    # 1. Validar somas por SKU+CANAL = 100%
    print("  üìä Validando somas por SKU+CANAL...")
    df_somas = (
        df
        .groupBy("SKU", "CANAL")
        .agg(F.sum("PERCENTUAL").alias("SomaPercentual"))
    )
    
    # Verificar se todas as somas s√£o exatamente 100.000%
    somas_invalidas = df_somas.filter(F.abs(F.col("SomaPercentual") - 100.0) > 0.0001)
    qtd_somas_invalidas = somas_invalidas.count()
    
    if qtd_somas_invalidas > 0:
        print(f"  ‚ùå ERRO: {qtd_somas_invalidas} combina√ß√µes SKU+CANAL n√£o somam exatamente 100.000%")
        
        # Adicionar informa√ß√µes de filiais para diagn√≥stico
        print("  üìã Diagn√≥stico com informa√ß√µes de filiais:")
        df_lojas_info = (
            spark.table('data_engineering_prd.app_operacoesloja.roteirizacaolojaativa')
            .select("CdFilial", "NmFilial", "NmPorteLoja")
        )
        
        # Extrair CdFilial da coluna LOJA (formato: 0021_0XXXX ou 0099_0XXXX)
        df_com_diagnostico = (
            somas_invalidas
            .join(df, on=["SKU", "CANAL"], how="inner")
            .withColumn("CdFilial", F.regexp_extract(F.col("LOJA"), r"(\d+)$", 1).cast("int"))
            .join(df_lojas_info, on="CdFilial", how="left")
            .select("SKU", "CANAL", "SomaPercentual", "CdFilial", "NmFilial", "NmPorteLoja", "LOJA")
            .distinct()
            .orderBy("SKU", "CANAL")
        )
        
        df_com_diagnostico.show(10, truncate=False)
        return False
    else:
        print(f"  ‚úÖ Todas as {df_somas.count()} combina√ß√µes SKU+CANAL somam exatamente 100.000%")
    
    # 2. Validar unicidade de chaves SKU-LOJA-CANAL
    print("  üîë Validando unicidade de chaves SKU-LOJA-CANAL...")
    df_contagem = (
        df
        .groupBy("SKU", "LOJA", "CANAL")
        .agg(F.count("*").alias("QtdRegistros"))
    )
    
    chaves_duplicadas = df_contagem.filter(F.col("QtdRegistros") > 1)
    qtd_chaves_duplicadas = chaves_duplicadas.count()
    
    if qtd_chaves_duplicadas > 0:
        print(f"  ‚ùå ERRO: {qtd_chaves_duplicadas} chaves SKU-LOJA-CANAL duplicadas")
        chaves_duplicadas.show(10, truncate=False)
        return False
    else:
        print(f"  ‚úÖ Todas as {df_contagem.count()} chaves SKU-LOJA-CANAL s√£o √∫nicas")
    
    # 3. Validar que para cada SKU, ambos os canais est√£o presentes
    print("  üîÑ Validando presen√ßa de ambos os canais por SKU...")
    
    df_canais_por_sku = (
        df
        .groupBy("SKU")
        .agg(
            F.countDistinct("CANAL").alias("QtdCanais"),
            F.collect_list("CANAL").alias("Canais")
        )
    )
    
    skus_incompletos = df_canais_por_sku.filter(F.col("QtdCanais") != 2)
    qtd_skus_incompletos = skus_incompletos.count()
    
    if qtd_skus_incompletos > 0:
        print(f"  ‚ùå ERRO: {qtd_skus_incompletos} SKUs n√£o t√™m ambos os canais")
        skus_incompletos.show(10, truncate=False)
        return False
    else:
        print(f"  ‚úÖ Todos os {df_canais_por_sku.count()} SKUs t√™m ambos os canais")
    
    # 4. Validar que ambos os canais s√£o ONLINE e OFFLINE
    print("  üìã Validando tipos de canais...")
    canais_unicos = df.select("CANAL").distinct().rdd.flatMap(lambda x: x).collect()
    canais_esperados = ["ONLINE", "OFFLINE"]
    
    if set(canais_unicos) != set(canais_esperados):
        print(f"  ‚ùå ERRO: Canais encontrados: {canais_unicos}, esperados: {canais_esperados}")
        return False
    else:
        print(f"  ‚úÖ Canais corretos: {canais_unicos}")
    
    print("  ‚úÖ Todas as valida√ß√µes passaram!")
    return True

def dividir_em_arquivos(df: DataFrame, categoria: str, max_linhas: int = MAX_LINHAS_POR_ARQUIVO) -> List[DataFrame]:
    """
    Divide DataFrame em arquivos garantindo que SKU-LOJA fique junto (ambos canais).
    
    Regra: Cada SKU-LOJA tem 2 registros (ONLINE + OFFLINE) que devem ficar no mesmo arquivo.
    
    Args:
        df: DataFrame completo
        max_linhas: M√°ximo de linhas por arquivo
        
    Returns:
        Lista de DataFrames
    """
    print(f"üîÑ Dividindo em arquivos (m√°x {max_linhas:,} linhas cada)...")
    
    # Validar integridade antes de dividir (com filtros aplicados)
    if not validar_integridade_dados_com_filtros(df, categoria):
        raise ValueError("‚ùå Valida√ß√£o de integridade falhou. N√£o √© poss√≠vel dividir os arquivos.")
    
    # Criar chave √∫nica por SKU (todos os registros do mesmo SKU ficam juntos)
    df_com_chave = df.withColumn("chave_particao", F.col("SKU"))
    
    # Contar registros por SKU
    df_contagem = (
        df_com_chave
        .groupBy("chave_particao")
        .agg(F.count("*").alias("qtd_registros"))
    )
    
    # Calcular parti√ß√µes
    window_particao = W.orderBy("chave_particao").rowsBetween(W.unboundedPreceding, W.currentRow)
    
    df_com_particao = (
        df_contagem
        .withColumn("acumulado", F.sum("qtd_registros").over(window_particao))
        .withColumn("num_arquivo", (F.col("acumulado") / max_linhas).cast("int"))
    )
    
    # Join de volta
    df_final = (
        df_com_chave
        .join(df_com_particao.select("chave_particao", "num_arquivo"), on="chave_particao", how="left")
        .drop("chave_particao")
    )
    
    # Separar em DataFrames
    num_arquivos = df_final.select(F.max("num_arquivo")).collect()[0][0] + 1
    print(f"  ‚Ä¢ Total de arquivos necess√°rios: {num_arquivos}")
    
    dfs_separados = []
    for i in range(num_arquivos):
        df_arquivo = df_final.filter(F.col("num_arquivo") == i).drop("num_arquivo")
        qtd = df_arquivo.count()
        
        # Validar que cada arquivo tem pares completos de canais
        validar_pares_canais_arquivo(df_arquivo, i)
        
        print(f"    - Parte {i+1}: {qtd:,} linhas")
        dfs_separados.append(df_arquivo)
    
    return dfs_separados

def validar_pares_canais_arquivo(df_arquivo: DataFrame, num_arquivo: int) -> None:
    """
    Valida que cada arquivo tem todos os registros de cada SKU (todas as LOJAs e canais).
    
    Args:
        df_arquivo: DataFrame do arquivo espec√≠fico
        num_arquivo: N√∫mero do arquivo para logs
    """
    print(f"  üîç Validando arquivo {num_arquivo + 1}...")
    
    # Contar registros por SKU no arquivo
    df_skus_arquivo = (
        df_arquivo
        .groupBy("SKU")
        .agg(F.count("*").alias("QtdRegistros"))
    )
    
    # Verificar se todos os SKUs t√™m registros completos (pelo menos 2 canais por LOJA)
    # Esta valida√ß√£o garante que n√£o h√° SKUs "cortados" entre arquivos
    skus_incompletos = df_skus_arquivo.filter(F.col("QtdRegistros") < 2)
    qtd_incompletos = skus_incompletos.count()
    
    if qtd_incompletos > 0:
        print(f"    ‚ùå ERRO: Arquivo {num_arquivo + 1} tem {qtd_incompletos} SKUs com menos de 2 registros")
        skus_incompletos.show(5, truncate=False)
        raise ValueError(f"Arquivo {num_arquivo + 1} tem SKUs incompletos")
    else:
        print(f"    ‚úÖ Arquivo {num_arquivo + 1}: Todos os SKUs t√™m registros completos")
    
    # Verificar se os canais s√£o ONLINE e OFFLINE
    canais_arquivo = df_arquivo.select("CANAL").distinct().rdd.flatMap(lambda x: x).collect()
    canais_esperados = ["ONLINE", "OFFLINE"]
    
    if not all(canal in canais_arquivo for canal in canais_esperados):
        print(f"    ‚ö†Ô∏è  AVISO: Arquivo {num_arquivo + 1} n√£o tem ambos os canais: {canais_arquivo}")
    else:
        print(f"    ‚úÖ Arquivo {num_arquivo + 1}: Canais corretos: {canais_arquivo}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Fun√ß√£o Principal de Exporta√ß√£o

# COMMAND ----------

def exportar_matriz_csv(categoria: str, data_exportacao: str = None, formato: str = "xlsx") -> List[str]:
    """
    Exporta matriz de merecimento em formato CSV ou XLSX para uma categoria.
    
    Processo completo:
    1. Carregar OFFLINE e ONLINE
    2. Uni√£o dos canais
    3. Normalizar para 100.00% exato
    4. Adicionar informa√ß√µes de filiais
    5. Criar DataFrame final formatado
    6. Dividir em arquivos (max 200k linhas)
    7. Salvar arquivos no formato escolhido
    
    Args:
        categoria: Nome da categoria
        data_exportacao: Data de exporta√ß√£o (padr√£o: hoje)
        formato: Formato de exporta√ß√£o - "csv" ou "xlsx" (padr√£o: "xlsx")
        
    Returns:
        Lista de caminhos dos arquivos salvos
    """
    if data_exportacao is None:
        data_exportacao = DATA_ATUAL.strftime("%Y-%m-%d")
    
    print(f"üöÄ Iniciando exporta√ß√£o para: {categoria}")
    print("=" * 80)
    
    grupo_apelido = TABELAS_MATRIZ_MERECIMENTO[categoria]["grupo_apelido"]
    
    # Criar pasta
    pasta_data = f"{PASTA_OUTPUT}/{data_exportacao}"
    os.makedirs(pasta_data, exist_ok=True)
    
    # 1. Carregar canais
    df_offline = carregar_e_filtrar_matriz(categoria, "offline")
    df_online = carregar_e_filtrar_matriz(categoria, "online")
    
    # 1.5. Diagn√≥stico de diferen√ßas
    diagnosticar_diferenca_canais(df_offline, df_online, categoria)
    
    # 2. Uni√£o
    print("\nüîó Unindo canais...")
    df_union = df_offline.union(df_online)
    print(f"  ‚úÖ Uni√£o: {df_union.count():,} registros")
    
    # 3. Adicionar informa√ß√µes de filiais (remover inativas)
    print()
    df_com_filiais = adicionar_informacoes_filial(df_union)
    
    # 4. Normalizar para 100.00% AP√ìS remo√ß√£o de filiais
    print()
    df_normalizado = normalizar_para_100_exato(df_com_filiais)
    
    # 5. Criar DataFrame final
    print()
    df_final = criar_dataframe_final(df_normalizado)
    
    # 6. Dividir em arquivos
    print()
    dfs_arquivos = dividir_em_arquivos(df_final, categoria)
    
    # 7. Salvar arquivos no formato escolhido
    print(f"\nüíæ Salvando arquivos {formato.upper()}...")
    arquivos_salvos = []
    
    for idx, df_arquivo in enumerate(dfs_arquivos, start=1):
        nome_base = f"matriz_merecimento_{grupo_apelido}_{data_exportacao}_parte{idx}"
        
        # Converter para Pandas
        df_pandas = df_arquivo.toPandas()
        
        # Garantir que PERCENTUAL seja float
        df_pandas["PERCENTUAL"] = df_pandas["PERCENTUAL"].astype(float)
        
        if formato.lower() == "csv":
            # Salvar CSV com v√≠rgula como separador decimal
            caminho_arquivo = f"{pasta_data}/{nome_base}.csv"
            df_pandas.to_csv(caminho_arquivo, index=False, sep=";", decimal=",", encoding="utf-8")
        elif formato.lower() == "xlsx":
            # Salvar XLSX
            caminho_arquivo = f"{pasta_data}/{nome_base}.xlsx"
            df_pandas.to_excel(caminho_arquivo, index=False, engine="openpyxl")
        else:
            raise ValueError(f"Formato '{formato}' n√£o suportado. Use 'csv' ou 'xlsx'.")
        
        print(f"  ‚úÖ Parte {idx}: {nome_base}.{formato.lower()} ({len(df_pandas):,} linhas)")
        arquivos_salvos.append(caminho_arquivo)
        
        print("\n" + "=" * 80)
    print(f"‚úÖ Exporta√ß√£o conclu√≠da: {categoria}")
    print(f"üìÅ Total de arquivos: {len(arquivos_salvos)}")
        
    return arquivos_salvos
        

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Exportar Todas as Categorias

# COMMAND ----------

def exportar_todas_categorias(data_exportacao: str = None, formato: str = "xlsx") -> Dict[str, List[str]]:
    """
    Exporta matrizes para todas as categorias no formato escolhido.
    
    Args:
        data_exportacao: Data de exporta√ß√£o (padr√£o: hoje)
        formato: Formato de exporta√ß√£o - "csv" ou "xlsx" (padr√£o: "xlsx")
        
    Returns:
        Dicion√°rio com listas de arquivos por categoria
    """
    print("üöÄ Iniciando exporta√ß√£o para TODAS as categorias")
    print("=" * 80)
    
    resultados = {}
    
    for categoria in TABELAS_MATRIZ_MERECIMENTO.keys():
        print(f"\nüìä Processando: {categoria}")
        print("-" * 60)
        
        try:
            arquivos = exportar_matriz_csv(categoria, data_exportacao, formato)
            resultados[categoria] = arquivos
        except Exception as e:
            print(f"‚ùå Erro: {str(e)}")
            resultados[categoria] = []
    
    print("\n" + "=" * 80)
    print("üìã RESUMO FINAL:")
    print("=" * 80)
    
    for categoria, arquivos in resultados.items():
        if arquivos:
            print(f"‚úÖ {categoria}: {len(arquivos)} arquivo(s)")
        else:
            print(f"‚ùå {categoria}: ERRO")
    
    
    return resultados

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Exporta√ß√£o Excel para Valida√ß√£o (por Grupo de Necessidade)

# COMMAND ----------

def exportar_excel_validacao_grupo_necessidade(categoria: str, data_exportacao: str = None) -> str:
    """
    Exporta Excel de valida√ß√£o com ONLINE e OFFLINE lado a lado.
    
    Estrutura:
    - Uma linha por (grupo_de_necessidade, CdFilial) - DISTINCT
    - Merecimentos s√£o SOMADOS por grupo + filial
    - Colunas: grupo_de_necessidade, CdFilial, Merecimento_OFFLINE, Merecimento_ONLINE
    - Fill com 0.00 em merecimentos faltantes
    - Exportado em pasta 'validacao'
    
    Args:
        categoria: Nome da categoria
        data_exportacao: Data de exporta√ß√£o (padr√£o: hoje)
        
    Returns:
        Caminho do arquivo Excel gerado
    """
    if data_exportacao is None:
        data_exportacao = DATA_ATUAL.strftime("%Y-%m-%d")
    
    print(f"üìä Exportando Excel de valida√ß√£o: {categoria}")
    print("=" * 80)
    
    grupo_apelido = TABELAS_MATRIZ_MERECIMENTO[categoria]["grupo_apelido"]
    
    # Criar pasta validacao
    pasta_validacao = f"{PASTA_OUTPUT}/{data_exportacao}/validacao"
    os.makedirs(pasta_validacao, exist_ok=True)
    
    # 1. Carregar dados OFFLINE com grupo_de_necessidade
    print("\nüîÑ Carregando matriz OFFLINE...")
    tabela_offline = TABELAS_MATRIZ_MERECIMENTO[categoria]["offline"]
    coluna_merecimento = COLUNAS_MERECIMENTO[categoria]
    
    df_offline = (
        spark.table(tabela_offline)
        .select(
            "CdSku", "CdFilial", "grupo_de_necessidade",
            (100 * F.col(coluna_merecimento)).alias("Merecimento_OFFLINE")
        )
    )
    
    # Filtro especial para Linha Leve: apenas SKUs das esp√©cies top 80% de PORTATEIS
    if categoria == "DIRETORIA LINHA LEVE":
        df_offline = df_offline.filter(F.col("CdSku").isin(skus_especies_top80))
        print(f"  ‚úÖ OFFLINE (TOP 80%): {df_offline.count():,} registros | {len(skus_especies_top80)} SKUs")
    else:
        print(f"  ‚úÖ OFFLINE: {df_offline.count():,} registros")
    
    # Agregar por grupo_de_necessidade + CdFilial (first merecimento, n√£o soma)
    df_offline_agg = (
        df_offline
        .groupBy("grupo_de_necessidade", "CdFilial")
        .agg(F.first("Merecimento_OFFLINE").alias("Merecimento_OFFLINE"))
    )
    print(f"  ‚úÖ OFFLINE agregado: {df_offline_agg.count():,} registros (grupo + filial)")
    
    # 2. Carregar dados ONLINE com grupo_de_necessidade
    print("\nüîÑ Carregando matriz ONLINE...")
    tabela_online = TABELAS_MATRIZ_MERECIMENTO[categoria]["online"]
    
    df_online = (
        spark.table(tabela_online)
        .select(
            "CdSku", "CdFilial", "grupo_de_necessidade", "NmPorteLoja",
            (100 * F.col(coluna_merecimento)).alias("Merecimento_ONLINE")
        )
        # Aplicar regra CdFilial 1401 ‚Üí 14 (apenas para TELAS e TELEFONIA)
        .withColumn(
            "CdFilial", 
            F.when(
                (F.col("CdFilial") == 1401) & (F.lit(categoria).isin(["DIRETORIA DE TELAS", "DIRETORIA TELEFONIA CELULAR"])), 
                14
            ).otherwise(F.col("CdFilial"))
        )
        .drop("NmPorteLoja")
    )
    
    # Filtro especial para Linha Leve: apenas SKUs das esp√©cies top 80% de PORTATEIS
    if categoria == "DIRETORIA LINHA LEVE":
        df_online = df_online.filter(F.col("CdSku").isin(skus_especies_top80))
        print(f"  ‚úÖ ONLINE (TOP 80%): {df_online.count():,} registros | {len(skus_especies_top80)} SKUs")
    else:
        print(f"  ‚úÖ ONLINE: {df_online.count():,} registros")
    
    # Agregar por grupo_de_necessidade + CdFilial (first merecimento, n√£o soma)
    df_online_agg = (
        df_online
        .groupBy("grupo_de_necessidade", "CdFilial")
        .agg(F.first("Merecimento_ONLINE").alias("Merecimento_ONLINE"))
    )
    
    print(f"  ‚úÖ ONLINE agregado: {df_online_agg.count():,} registros (grupo + filial)")
    
    # 3. Fazer FULL OUTER JOIN
    print("\nüîó Fazendo outer join...")
    df_joined = (
        df_offline_agg.join(
            df_online_agg,
            on=["grupo_de_necessidade", "CdFilial"],
            how="outer"
        )
        # Fill NULLs com 0.00 para merecimentos
        .fillna(0.00, subset=["Merecimento_OFFLINE", "Merecimento_ONLINE"])
        .orderBy("grupo_de_necessidade", "CdFilial")
    )
    print(f"  ‚úÖ Join: {df_joined.count():,} registros")
    
    # 4. Converter para Pandas e salvar Excel
    print("\nüíæ Salvando Excel...")
    df_pandas = df_joined.toPandas()
    
    # Reordenar colunas para melhor visualiza√ß√£o
    colunas_ordenadas = ["grupo_de_necessidade", "CdFilial", "Merecimento_OFFLINE", "Merecimento_ONLINE"]
    df_pandas = df_pandas[colunas_ordenadas]
    
    # Arredondar merecimentos para 3 casas decimais
    df_pandas["Merecimento_OFFLINE"] = df_pandas["Merecimento_OFFLINE"].round(3)
    df_pandas["Merecimento_ONLINE"] = df_pandas["Merecimento_ONLINE"].round(3)
    
    # Salvar
    nome_arquivo = f"validacao_{grupo_apelido}_{data_exportacao}.xlsx"
    caminho_completo = f"{pasta_validacao}/{nome_arquivo}"
    
    # Salvar diretamente (mais robusto para DataFrames grandes)
    try:
        df_pandas.to_excel(caminho_completo, sheet_name="Validacao", index=False, engine="openpyxl")
        print(f"  ‚úÖ Arquivo salvo: {nome_arquivo}")
        print(f"  üìÅ Local: {pasta_validacao}")
        print(f"  üìä Total de linhas: {len(df_pandas):,}")
    except Exception as e:
        print(f"  ‚ö†Ô∏è Erro ao salvar Excel: {str(e)}")
        print(f"  üí° Tentando salvar como CSV...")
        caminho_csv = caminho_completo.replace(".xlsx", ".csv")
        df_pandas.to_csv(caminho_csv, index=False)
        print(f"  ‚úÖ Arquivo CSV salvo: {caminho_csv}")
        caminho_completo = caminho_csv
    
    print("\n" + "=" * 80)
    print(f"‚úÖ Exporta√ß√£o Excel de valida√ß√£o conclu√≠da: {categoria}")
    
    return caminho_completo

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Exportar Valida√ß√£o para Todas as Categorias

# COMMAND ----------

def exportar_excel_validacao_todas_categorias(data_exportacao: str = None) -> Dict[str, str]:
    """
    Exporta Excel de valida√ß√£o para todas as categorias.
    
    Args:
        data_exportacao: Data de exporta√ß√£o (padr√£o: hoje)
        
    Returns:
        Dicion√°rio com caminhos dos arquivos por categoria
    """
    print("üöÄ Iniciando exporta√ß√£o Excel de valida√ß√£o para TODAS as categorias")
    print("=" * 80)
    
    resultados = {}
    
    for categoria in TABELAS_MATRIZ_MERECIMENTO.keys():
        print(f"\nüìä Processando: {categoria}")
        print("-" * 60)
        
        try:
            arquivo = exportar_excel_validacao_grupo_necessidade(categoria, data_exportacao)
            resultados[categoria] = arquivo
        except Exception as e:
            print(f"‚ùå Erro: {str(e)}")
            resultados[categoria] = None
    
    print("\n" + "=" * 80)
    print("üìã RESUMO FINAL - VALIDA√á√ÉO:")
    print("=" * 80)
    
    for categoria, arquivo in resultados.items():
        if arquivo:
            print(f"‚úÖ {categoria}: {arquivo}")
        else:
            print(f"‚ùå {categoria}: ERRO")
    
    return resultados

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Execu√ß√£o

# COMMAND ----------

# Executar exporta√ß√£o para todas as categorias
resultados = exportar_todas_categorias()

# COMMAND ----------

# Exportar Excel de valida√ß√£o para todas as categorias
resultados_validacao = exportar_excel_validacao_todas_categorias()

# COMMAND ----------

# Exemplo: exportar apenas uma categoria
# arquivos = exportar_matriz_csv("DIRETORIA TELEFONIA CELULAR")

# Exemplo: exportar apenas valida√ß√£o de uma categoria
# arquivo_validacao = exportar_excel_validacao_grupo_necessidade("DIRETORIA TELEFONIA CELULAR")
